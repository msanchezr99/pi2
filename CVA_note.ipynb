{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min:  -18.589511615102026  min:  -40.45791106355474\n",
      "min:  -18.141677057558486  min:  -37.03979895407709\n",
      "min:  -17.801624276995216  min:  -37.930389259090475\n",
      "min:  -17.20333912216591  min:  -36.67659142088034\n",
      "min:  -18.59312671640936  min:  -37.44207857842168\n",
      "min:  -18.138089153708023  min:  -39.91782140840885\n",
      "min:  -17.691925088135985  min:  -36.60745044993907\n",
      "min:  -17.488590011527666  min:  -42.34308809155709\n",
      "min:  -17.734666662892383  min:  -36.03803250016985\n",
      "min:  -19.544066415420303  min:  -41.97847545258237\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    numpy_array = np.load(f'./output_arrays/epoch_{i}_channel_{i}.npy')\n",
    "    print(\"min: \",np.max(numpy_array),\" min: \", np.min(numpy_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 7680)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_freqs,n_times=numpy_array.shape\n",
    "n_freqs,n_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpyImageDataset(Dataset):\n",
    "    def __init__(self, image_shape, root_dir='./output_arrays/', transform=None):\n",
    "        \"\"\"\n",
    "        root_dir (string): Folder con archivos .npy.\n",
    "        transform (callable, optional)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.n_freqs,self.n_times=image_shape\n",
    "        self.transform = transform\n",
    "        self.image_files_names = [f for f in os.listdir(root_dir)]# if f.endswith('.npy')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load .npy file\n",
    "        img_path = os.path.join(self.root_dir, self.image_files_names[idx])\n",
    "        image = np.load(img_path)\n",
    "        \n",
    "        image = image.reshape(1, self.n_freqs, self.n_times)\n",
    "        # Convert image to a PyTorch tensor\n",
    "        image = torch.from_numpy(image).float()\n",
    "        \n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image # Retorna (input, ) pair for autoencoder\n",
    "        \n",
    "class MinMaxNormalize:\n",
    "    def __call__(self, image):\n",
    "        min_val = image.min()\n",
    "        max_val = image.max()\n",
    "        return (image - min_val) / (max_val - min_val + 1e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    #transforms.resize((n_freqs, n_times))\n",
    "    #transforms.Normalize((0.5,), (0.5,))\n",
    "    MinMaxNormalize()\n",
    "])\n",
    "\n",
    "dataset = NpyImageDataset((n_freqs,n_times), root_dir='./output_arrays/', transform=image_transforms) #input_shape=(n_freqs,n_times),\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=32,\n",
    "                        num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.storage of tensor([[[-40.7905, -40.7911, -40.7943,  ..., -38.1793, -38.1790, -38.1810],\n",
       "         [-40.8971, -40.8965, -40.8983,  ..., -38.3108, -38.3074, -38.3065],\n",
       "         [-40.9999, -40.9976, -40.9976,  ..., -38.4492, -38.4425, -38.4382],\n",
       "         ...,\n",
       "         [-46.4949, -46.5135, -46.5351,  ..., -42.9284, -42.9197, -42.9135],\n",
       "         [-46.3609, -46.3745, -46.3907,  ..., -42.9573, -42.9482, -42.9416],\n",
       "         [-46.1945, -46.2021, -46.2122,  ..., -42.9789, -42.9695, -42.9626]]])>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_shape, latent_dim=16,variational=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.variational=variational\n",
    "        n_freqs, n_times = input_shape\n",
    "\n",
    "        # Capas convolucionales\n",
    "        self.conv1 = nn.Conv2d(1, 12, kernel_size=(3,5), stride=2)\n",
    "        self.conv2 = nn.Conv2d(12, 20, kernel_size=(3,5), stride=2)\n",
    "        self.conv3 = nn.Conv2d(20, 30, kernel_size=(3,5), stride=2)\n",
    "        \n",
    "        # Determinación de shape luego de convolución\n",
    "        self.conv_c_out,self.conv_h_out,self.conv_w_out=self._get_conv_output(input_shape)\n",
    "        self.conv_output_size = self.conv_c_out*self.conv_h_out*self.conv_w_out\n",
    "        \n",
    "        # Capas fully connected para espacio latente\n",
    "        self.fc_mu = nn.Linear(self.conv_output_size, latent_dim)\n",
    "        if self.variational:\n",
    "            self.fc_logvar = nn.Linear(self.conv_output_size, latent_dim)\n",
    "\n",
    "    def _get_conv_output(self, input_shape):\n",
    "        \"\"\"Función auxiliar en cálculo de tamaño tras convoluciones\n",
    "        Retorna (C_out,H_out,W_out): numero de canales, altura y ancho de salida.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, 1, *input_shape)  # Create a dummy input with batch size 1\n",
    "            x = self.conv1(x)\n",
    "            x = self.conv2(x)\n",
    "            x = self.conv3(x)\n",
    "            #print(\"Numel\",x.numel(),  \"vs shape1xshape2xshape3: \",x.shape[1]*x.shape[2]*x.shape[3],\"shape\", x.shape)\n",
    "            return x.shape[1],x.shape[2],x.shape[3]\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        # Flatten the output\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        print(\"Encoder Forward: Flatten alcanzado. x.hape: \",x.shape, \"Predicted output size: \", self.conv_output_size)\n",
    "    \n",
    "\n",
    "        # Obtener parámetros mu, logvarianza\n",
    "        mu = self.fc_mu(x)\n",
    "        print(\"Mu alcanzado\",x.shape)\n",
    "        if self.variational:\n",
    "            logvar = self.fc_logvar(x)\n",
    "            return mu,logvar\n",
    "        return mu\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_shape, encoder_conv_out_shape,latent_dim=16,):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_freqs, self.n_times = input_shape\n",
    "\n",
    "        # Shape luego de convoluciones en el encoder para hacer reshaping\n",
    "        self.conv_output_shape = encoder_conv_out_shape\n",
    "        \n",
    "        # Fully connected layer for reconstructing feature map shape\n",
    "        self.fc = nn.Linear(latent_dim, self.conv_output_shape[0]*self.conv_output_shape[1]*self.conv_output_shape[2])#C*H*W\n",
    "        \n",
    "        # Deconvolution layers\n",
    "        self.deconv1 = nn.ConvTranspose2d(30, 20, kernel_size=3, stride=2)\n",
    "        self.deconv2 = nn.ConvTranspose2d(20, 12, kernel_size=3, stride=2)\n",
    "        self.deconv3 = nn.ConvTranspose2d(12, 1, kernel_size=3, stride=2)\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Decode fully connected to a feature map\n",
    "        x = self.fc(z)\n",
    "        x = x.view(-1, self.conv_output_shape[0], self.conv_output_shape[1], self.conv_output_shape[2])# C(canales), H, W\n",
    "        \n",
    "        # Apply deconvolutions\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        \n",
    "        x = F.relu(self.deconv2(x))\n",
    "        print(\"Decoder 1\",x.shape)\n",
    "        x = torch.sigmoid(self.deconv3(x))  # Output in [0, 1] range\n",
    "        \n",
    "        return x\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_shape, latent_dim=16,variational=False):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.variational=variational\n",
    "        self.encoder = Encoder(input_shape, latent_dim,variational=variational)\n",
    "        encoder_conv_h_w_out=self.encoder.conv_c_out,self.encoder.conv_h_out,self.encoder.conv_w_out\n",
    "        print(\"Instanciado encoder\")\n",
    "        self.decoder = Decoder(input_shape,encoder_conv_h_w_out, latent_dim)\n",
    "        print(\"Instanciado decoder\")\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)  # Standard deviation\n",
    "        eps = torch.randn_like(std)  # Random noise\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.variational:# Encode\n",
    "            mu, logvar = self.encoder(x)\n",
    "            # Reparameterize\n",
    "            z = self.reparameterize(mu, logvar)\n",
    "            # Decode\n",
    "            reconstructed = self.decoder(z)\n",
    "            return reconstructed, mu, logvar\n",
    "        else: \n",
    "            z=self.encoder(x)\n",
    "            reconstructer=self.decoder(z)\n",
    "            return reconstructed\n",
    "# Define loss function\n",
    "def loss_funct(reconstructed, original, mu=None, logvar=None, kld_weight=0.1,variational=False):\n",
    "    \"\"\"\n",
    "    Compute VAE loss with weighted KL divergence\n",
    "    \"\"\"\n",
    "    recon_loss = F.binary_cross_entropy(reconstructed, original, reduction='sum')\n",
    "    if variational:\n",
    "        kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return recon_loss + kld_weight * kld_loss\n",
    "    else:\n",
    "        return recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numel 459360 vs shape1xshape2xshape3:  459360 shape torch.Size([1, 30, 16, 957])\n",
      "Instanciado encoder\n",
      "Instanciado decoder\n"
     ]
    }
   ],
   "source": [
    "# import CVA\n",
    "input_shape = (n_freqs, n_times) \n",
    "latent_dim = 16\n",
    "variational=True\n",
    "\n",
    "if variational:\n",
    "    model = VariationalAutoencoder(input_shape=input_shape, latent_dim=latent_dim,variational=True)\n",
    "else: \n",
    "    model = VariationalAutoencoder(input_shape=input_shape, latent_dim=latent_dim,variational=False)    \n",
    "learning_rate = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 12, 69, 3838])\n",
      "torch.Size([32, 20, 34, 1917])\n",
      "torch.Size([32, 30, 16, 957])\n",
      "Flatten alcanzado torch.Size([32, 459360]) Predicted output size:  459360\n",
      "Mu alcanzado torch.Size([32, 459360])\n",
      "Decoder 1 torch.Size([32, 12, 67, 3831])\n",
      "Epoch [1/10], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10  \n",
    "\n",
    "for epoch in range(num_epochs):#training epochs\n",
    "    model.train()  \n",
    "    total_loss = 0\n",
    "    \n",
    "    for images in dataloader:  # batch\n",
    "        # Forward\n",
    "        reconstructed, mu, logvar = model(images)     \n",
    "        #\n",
    "        #print(logvar,type(logvar))\n",
    "          \n",
    "        loss = loss_funct(reconstructed, images, mu, logvar,variational=True)\n",
    "        # Backward pass and optimization step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Track the loss\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Print average loss per epoch\n",
    "    average_loss = total_loss / len(dataloader.dataset)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 16])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logvar.shape"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
